{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "q7phyOVFQPxg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7phyOVFQPxg",
    "outputId": "154b02b8-201e-4898-fd40-ca6a8a2fff40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade networkx==2.6\n",
    "#!pip install --upgrade scipy==1.8.0\n",
    "#!pip install praw\n",
    "#!pip install stanza\n",
    "#!pip install --upgrade scikit-learn\n",
    "#!pip install wordcloud\n",
    "#!pip install scikit-optimize\n",
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb952c87",
   "metadata": {
    "id": "eb952c87"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import praw\n",
    "import nltk\n",
    "import time\n",
    "import math\n",
    "import stanza\n",
    "import string\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "#use scipy v1.8.0, networkx v2.6\n",
    "import networkx as nx \n",
    "\n",
    "from time import time\n",
    "from heapq import nlargest\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score,  classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d977f",
   "metadata": {
    "id": "c83d977f"
   },
   "source": [
    "<h3>2.1. Data Cleaning and Splitting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "jWUTFNhaUPYf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "jWUTFNhaUPYf",
    "outputId": "5b7aed47-3c20-4eb9-fb0a-0c0165769155"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('to_label.csv')\n",
    "df_2 = pd.read_csv('to_label_2.csv')\n",
    "df_3 = pd.read_csv('to_label_3.csv')\n",
    "df_4 = pd.read_csv('to_label_4.csv')\n",
    "df_5 = pd.read_csv('to_label_4.csv')\n",
    "\n",
    "#merge all labelled dataframes\n",
    "df = pd.concat([df, df_2, df_3, df_4, df_5], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3d55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "pattern = r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "#remove comments with only hyperlinks\n",
    "df = df[~df['content'].str.contains(pattern)]\n",
    "#remove comments which were deleted\n",
    "df = df.loc[df['content'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "145c4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some more stop words ontop of the default stop word list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    #define regex for urls\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url_pattern.sub('', text) # Remove URLs\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Return cleaned text after removing Nones\n",
    "    tokens = list(filter(lambda x: x is not None, tokens))\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1623d07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprocess all comments/posts into new column\n",
    "df['cleaned_content'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b436c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "# Split comments into sentences and perform POS tagging on each sentence\n",
    "sentences = [nltk.sent_tokenize(content) for content in df['cleaned_content']]\n",
    "pos_tagged_sentences = [[nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in comment] for comment in sentences]\n",
    "\n",
    "# Extract POS features for each comment; idea is to count the number of tokens belonging to each\n",
    "#pos tag within each sentence\n",
    "pos_features = []\n",
    "for content in pos_tagged_sentences:\n",
    "    pos_dict = {}\n",
    "    for sentence in content:\n",
    "        for word, tag in sentence:\n",
    "            pos_dict[tag] = pos_dict.get(tag, 0) + 1\n",
    "    pos_features.append(pos_dict)\n",
    "    \n",
    "# Convert POS features to a sparse matrix\n",
    "pos_vectorizer = DictVectorizer(sparse=False)\n",
    "pos_features = pos_vectorizer.fit_transform(pos_features)\n",
    "\n",
    "# Define a function for stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "#fit tfidf_vectorizer with corpus\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words = 'english',\n",
    "    ngram_range = (1, 2),\n",
    "    tokenizer = tokenize_and_stem,\n",
    "    min_df = 2, #ignore if ngram appears in less than 2 documents\n",
    "    max_df = 0.9 #ignore if ngram appears in more than 89% of the documents\n",
    ")\n",
    "\n",
    "tfidf_features = tfidf.fit_transform(df['cleaned_content'])\n",
    "\n",
    "#fit countvectorizer with corpus\n",
    "bow = CountVectorizer()\n",
    "bow_features = bow.fit_transform(df['cleaned_content'])\n",
    "\n",
    "#combine tfidf, bow  and pos features into a single array\n",
    "tfidf_pos = np.concatenate((tfidf_features.toarray(), bow_features.toarray(), pos_features), axis = 1)\n",
    "\n",
    "#add score to the array\n",
    "tfidf_pos = np.c_[tfidf_pos, df['score']]\n",
    "\n",
    "#specify encoding sequence\n",
    "sequence = ['negative', 'neutral', 'positive']\n",
    "\n",
    "#load label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#encode labels\n",
    "labels = le.fit(sequence).transform(df['sentiment'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29f3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_pos,\n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc4e84",
   "metadata": {},
   "source": [
    "<h3> 2.2. Random Forest Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be7bcea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "              n_jobs=-1, random_state=42, scoring='f1_weighted',\n",
       "              search_spaces={'max_features': ['sqrt', 'log2'],\n",
       "                             'n_estimators': [30, 40, 50, 60, 70, 80, 90]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize randomforest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "#gridsearch to determine optimal n_estimator with cross-validation; best params: max_features= sqrt, n_estimators = 80\n",
    "params_rf = {\n",
    "    'n_estimators': list(range(30, 100, 10)),\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "gs = BayesSearchCV(\n",
    "    n_jobs = -1,\n",
    "    estimator = classifier,\n",
    "    search_spaces = params_rf,\n",
    "    cv = 5,\n",
    "    scoring = 'f1_weighted',\n",
    "    random_state= 42\n",
    ")\n",
    "\n",
    "#start time\n",
    "start = time()\n",
    "\n",
    "#fit\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f10997e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('max_features', 'sqrt'), ('n_estimators', 90)])\n",
      "took 314.822 seconds\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  41                  66                  11\n",
      "true negative                  5                 206                  17\n",
      "true positive                  2                  40                  86\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.35      0.49       118\n",
      "           1       0.66      0.90      0.76       228\n",
      "           2       0.75      0.67      0.71       128\n",
      "\n",
      "    accuracy                           0.70       474\n",
      "   macro avg       0.76      0.64      0.66       474\n",
      "weighted avg       0.73      0.70      0.68       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and test F1 score\n",
    "print(f\"Best parameters: {gs.best_params_}\")\n",
    "\n",
    "#score\n",
    "y_pred = gs.predict(X_test)\n",
    "print(f\"took {time() - start:.3f} seconds\")\n",
    "\n",
    "#confusion Matrix\n",
    "cf = pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76456ab5",
   "metadata": {},
   "source": [
    "<h3> 2.3. Logistic Regression Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b7b0297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('C', 0.1)])\n",
      "[penalty type - none] f1 test: 0.5148194539186893 | took 496.028 seconds\n",
      "Best parameters: OrderedDict([('C', 10.0)])\n",
      "[penalty type - l1] f1 test: 0.6749020729219407 | took 33.964 seconds\n",
      "Best parameters: OrderedDict([('C', 100.0)])\n",
      "[penalty type - l2] f1 test: 0.5492880375397172 | took 492.680 seconds\n"
     ]
    }
   ],
   "source": [
    "penalty_pairs = {\n",
    "    'none': 'none',\n",
    "    'l1': 'liblinear',\n",
    "    'l2': 'lbfgs'\n",
    "}\n",
    "\n",
    "#define parameters\n",
    "params_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "best_model_lr = \"\"\n",
    "best_f1 = 0\n",
    "\n",
    "#iterate through each penalty and solver pair\n",
    "for p, s in penalty_pairs.items():\n",
    "    if p == 'none':\n",
    "        logreg = LogisticRegression(penalty = 'none')\n",
    "    else:\n",
    "        logreg = LogisticRegression(penalty = p, solver = s)\n",
    "\n",
    "    gs_logreg = BayesSearchCV(\n",
    "        n_jobs = -1,\n",
    "        estimator = logreg,\n",
    "        search_spaces = params_lr,\n",
    "        cv = 5,\n",
    "        scoring = 'f1_weighted',\n",
    "        random_state= 42\n",
    "    )\n",
    "\n",
    "    #start time\n",
    "    start = time()\n",
    "    #fit gridsearch to training data\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and test F1 score\n",
    "    print(f\"Best parameters: {gs_logreg.best_params_}\")\n",
    "\n",
    "\n",
    "    #score\n",
    "    y_pred = gs_logreg.predict(X_test)\n",
    "    \n",
    "    f1_test = f1_score(y_test, y_pred, average = 'weighted')\n",
    "\n",
    "    #weighted - puts more emphasis on classes with lower data points\n",
    "    print(f\"[penalty type - {p}] f1 test: {f1_test} | took {time() - start:.3f} seconds\")\n",
    "    \n",
    "    if f1_test > best_f1:\n",
    "        best_model_lr = gs_logreg\n",
    "        best_f1 = f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4846b579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('C', 10.0)])\n",
      "took 492.699 seconds\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  56                  45                  17\n",
      "true negative                 20                 183                  25\n",
      "true positive                 14                  30                  84\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.47      0.54       118\n",
      "           1       0.71      0.80      0.75       228\n",
      "           2       0.67      0.66      0.66       128\n",
      "\n",
      "    accuracy                           0.68       474\n",
      "   macro avg       0.67      0.64      0.65       474\n",
      "weighted avg       0.68      0.68      0.67       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and test F1 score\n",
    "print(f\"Best parameters: {best_model_lr.best_params_}\")\n",
    "\n",
    "#score\n",
    "y_pred = best_model_lr.predict(X_test)\n",
    "print(f\"took {time() - start:.3f} seconds\")\n",
    "\n",
    "#confusion Matrix\n",
    "cf = pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a9e12",
   "metadata": {},
   "source": [
    "<h3> 2.4. XGBoost Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "879c26ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#start time\n",
    "start = time()\n",
    "\n",
    "#train the model\n",
    "xgbm = XGBClassifier(n_estimators = 100)\n",
    "\n",
    "#define parameters; result: best max_depth is 9, learning rate is 0.9\n",
    "params_xgbm = {\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'learning_rate': Real(0.001, 1.0, prior='log-uniform'),\n",
    "    'reg_alpha': Real(1e-9, 1.0, prior='log-uniform'),\n",
    "}\n",
    "\n",
    "gs_xgbm = BayesSearchCV(\n",
    "    n_jobs = -1,\n",
    "    estimator = xgbm,\n",
    "    search_spaces = params_xgbm,\n",
    "    cv = 5,\n",
    "    scoring = 'f1_weighted',\n",
    "    random_state= 42\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d246593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('learning_rate', 0.9893064224892779), ('max_depth', 3), ('reg_alpha', 0.0205428306991576)])\n",
      "took 7492.519 seconds\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  62                  39                  17\n",
      "true negative                 27                 176                  25\n",
      "true positive                 14                  21                  93\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.53      0.56       118\n",
      "           1       0.75      0.77      0.76       228\n",
      "           2       0.69      0.73      0.71       128\n",
      "\n",
      "    accuracy                           0.70       474\n",
      "   macro avg       0.68      0.67      0.68       474\n",
      "weighted avg       0.69      0.70      0.70       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and test F1 score\n",
    "print(f\"Best parameters: {gs_xgbm.best_params_}\")\n",
    "\n",
    "#score\n",
    "y_pred = gs_xgbm.predict(X_test)\n",
    "print(f\"took {time() - start:.3f} seconds\")\n",
    "\n",
    "#confusion Matrix\n",
    "cf = pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd1f95",
   "metadata": {},
   "source": [
    "<h3> 2.5. MLP Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f65ae45f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=MLPClassifier(learning_rate='adaptive',\n",
       "                                      random_state=42),\n",
       "              n_jobs=-1, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh', 'logistic'],\n",
       "                             'hidden_layer_sizes': [10, 20, 30, 60, 120]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize model\n",
    "mlp = MLPClassifier(random_state=42, learning_rate = 'adaptive')\n",
    "\n",
    "#define parameters\n",
    "params_mlp = {\n",
    "    'hidden_layer_sizes': [10, 20, 30, 60, 120],\n",
    "    'activation': ['relu', 'tanh', 'logistic']\n",
    "}\n",
    "\n",
    "gs_mlp = BayesSearchCV(\n",
    "    estimator = mlp, \n",
    "    search_spaces = params_mlp, \n",
    "    cv=5, \n",
    "    n_jobs = -1, \n",
    "    random_state= 42\n",
    ")\n",
    "\n",
    "#start time\n",
    "start = time()\n",
    "\n",
    "#fit gridsearch to training data\n",
    "gs_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4bf2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('activation', 'logistic'), ('hidden_layer_sizes', 20)])\n",
      "took 3926.872 seconds\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  61                  37                  20\n",
      "true negative                 36                 163                  29\n",
      "true positive                 14                  28                  86\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.52      0.53       118\n",
      "           1       0.71      0.71      0.71       228\n",
      "           2       0.64      0.67      0.65       128\n",
      "\n",
      "    accuracy                           0.65       474\n",
      "   macro avg       0.63      0.63      0.63       474\n",
      "weighted avg       0.65      0.65      0.65       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and test F1 score\n",
    "print(f\"Best parameters: {gs_mlp.best_params_}\")\n",
    "\n",
    "#score\n",
    "y_pred = gs_mlp.predict(X_test)\n",
    "print(f\"took {time() - start:.3f} seconds\")\n",
    "\n",
    "#confusion Matrix\n",
    "cf = pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5762d",
   "metadata": {},
   "source": [
    "<h3> 2.6. Compare against Pre-tuned Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c835349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check against vader's performance\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['content'], \n",
    "    labels, \n",
    "    test_size = 0.2, \n",
    "    random_state= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8c06a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score: 0.30695827836035205\n"
     ]
    }
   ],
   "source": [
    "#empty lists to store output\n",
    "vader_labels = []\n",
    "\n",
    "#initialize analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#iterate and label each comment/post\n",
    "for i, comment in enumerate(X_test):\n",
    "    sent = analyzer.polarity_scores(comment)\n",
    "    compound_score = sent['compound']\n",
    "    \n",
    "    #introduce a threshold of -0.05~0.05 for 'neutral' comments; \n",
    "    #-1 is very negative, 0 is very neutral, 1 is very positive\n",
    "    if compound_score > 0.05:\n",
    "        sent = 2\n",
    "    elif compound_score < -0.05:\n",
    "        sent = 1\n",
    "    else:\n",
    "        sent = 0\n",
    "    \n",
    "    #store output\n",
    "    vader_labels.append(sent)\n",
    "    \n",
    "#view weighted f1 score\n",
    "print(f\"Test F1 score: {f1_score(y_test, vader_labels, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb8a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  16                  49                  53\n",
      "true negative                 91                  44                  93\n",
      "true positive                 14                  12                 102\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.14      0.13       118\n",
      "           1       0.42      0.19      0.26       228\n",
      "           2       0.41      0.80      0.54       128\n",
      "\n",
      "    accuracy                           0.34       474\n",
      "   macro avg       0.32      0.38      0.31       474\n",
      "weighted avg       0.35      0.34      0.31       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion Matrix\n",
    "cf = pd.DataFrame(\n",
    "        confusion_matrix(y_test, vader_labels),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "\n",
    "print(cf)\n",
    "\n",
    "#classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, vader_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "128e19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare against stanza's sentiment analyzer\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['content'], \n",
    "    labels, \n",
    "    test_size = 0.2, \n",
    "    random_state= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08639684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 13:10:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f91d9d5dc7471cbb6cd740e46034f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7940e5b5731e4f8d82e8866b08057396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/tokenize/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721ecc9692f0411990f7aa72649d5923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/sentiment/sstplus.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f89b8221fc42b9ab52ddeca6f3e5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/pretrain/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b8cc9507a544df94410003ae234a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/forward_charlm/1billion.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c73cbc879854dfc82a1f7126367bd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.1/models/backward_charlm/1billion.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 13:10:20 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2023-03-28 13:10:20 INFO: Use device: gpu\n",
      "2023-03-28 13:10:20 INFO: Loading: tokenize\n",
      "2023-03-28 13:10:24 INFO: Loading: sentiment\n",
      "2023-03-28 13:10:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize, sentiment')\n",
    "\n",
    "# define a function to classify sentiment of a comment\n",
    "def classify_sentiment(comment):\n",
    "    # analyze the sentiment of the comment\n",
    "    doc = nlp(comment)\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        if sentence.sentiment == 0:\n",
    "            return 1\n",
    "        elif sentence.sentiment == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "# classify the sentiment of each comment\n",
    "stanza_pred = [classify_sentiment(comment) for comment in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21e52dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  42                  65                  11\n",
      "true negative                159                  46                  23\n",
      "true positive                 48                  26                  54\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.36      0.23       118\n",
      "           1       0.34      0.20      0.25       228\n",
      "           2       0.61      0.42      0.50       128\n",
      "\n",
      "    accuracy                           0.30       474\n",
      "   macro avg       0.37      0.33      0.33       474\n",
      "weighted avg       0.37      0.30      0.31       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion Matrix\n",
    "cf = pd.DataFrame(\n",
    "        confusion_matrix(y_test, stanza_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "\n",
    "print(cf)\n",
    "\n",
    "#classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, stanza_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976683ac",
   "metadata": {},
   "source": [
    "<h3> 3. Labelling the Main Dataset with the Best Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae7c168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##summary table of the optimum xgboost\n",
    "#form xgboost model with the optimum parameters\n",
    "xgbm = XGBClassifier(n_estimators = 100, max_depth = 9, learning_rate = 0.9).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f4d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same preprocessing\n",
    "main_df = pd.read_csv('top_posts_year.csv')\n",
    "\n",
    "#data cleaning\n",
    "pattern = r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "#remove comments with only hyperlinks\n",
    "main_df = main_df[~main_df['content'].str.contains(pattern)]\n",
    "#remove comments which were deleted\n",
    "main_df = main_df.loc[main_df['content'] != '[deleted]']\n",
    "\n",
    "#preprocess all comments/posts into new column\n",
    "main_df['cleaned_content'] = main_df['content'].apply(preprocess_text)\n",
    "\n",
    "# Split comments into sentences and perform POS tagging on each sentence\n",
    "sentences = [nltk.sent_tokenize(content) for content in main_df['cleaned_content']]\n",
    "pos_tagged_sentences = [[nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in comment] for comment in sentences]\n",
    "\n",
    "# Extract POS features for each comment; idea is to count the number of tokens belonging to each\n",
    "#pos tag within each sentence\n",
    "pos_features = []\n",
    "for content in pos_tagged_sentences:\n",
    "    pos_dict = {}\n",
    "    for sentence in content:\n",
    "        for word, tag in sentence:\n",
    "            pos_dict[tag] = pos_dict.get(tag, 0) + 1\n",
    "    pos_features.append(pos_dict)\n",
    "    \n",
    "# Convert POS features to a sparse matrix\n",
    "pos_features = pos_vectorizer.transform(pos_features)\n",
    "tfidf_features = tfidf.transform(main_df['cleaned_content'])\n",
    "bow_features = bow.transform(main_df['cleaned_content'])\n",
    "\n",
    "#combine tfidf, bow  and pos features into a single array\n",
    "tfidf_pos = np.concatenate((tfidf_features.toarray(), bow_features.toarray(), pos_features), axis = 1)\n",
    "\n",
    "#add score to the array\n",
    "tfidf_pos = np.c_[tfidf_pos, main_df['score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3976ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "sents = []\n",
    "for i in xgbm.predict(tfidf_pos):\n",
    "    sent = ''\n",
    "    if i == 0:\n",
    "        sent = 'negative'\n",
    "    elif i == 1:\n",
    "        sent = 'neutral'\n",
    "    else:\n",
    "        sent = 'positive'\n",
    "    sents.append(sent)\n",
    "    \n",
    "main_df['pred_sentiment'] = sents\n",
    "\n",
    "#save dont run as it'll overwrite the current file\n",
    "#main_df.to_csv('predict_output.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
